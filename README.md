# DSChallengeMELI

## üìå Requerimientos
Una vez clonado el repositorio, es necesario:

- instalar [python](https://www.python.org/downloads/)
- abrir una terminal, ubicarse dentro de la carpeta **DSChallengeMELI** y correr `pip install -r requirements.txt`
- correr el script `project_set_up.py`: genera la estructura de carpetas necesaria para que no se generen inconvenientes al insertar dentro de los notebooks en carpetas inexistentes 

## üìå Notas

- Los notebooks asociados a cada inciso estan numerados indicando el orden en que deber√≠a correrse cada uno
- Tom√© ciertas decisiones teniendo en cuenta:
  - recursos / poder de c√≥mputo (16Gb RAM, procesador i5)
  - no quer√≠a demorarme demasiado en la entrega de la resoluci√≥n 
 
Es por eso que enumero en √©ste documento algunas de las ideas que se me ocurrieron y que podr√≠a probar en cada caso.
 

## üìå Exploracion y analisis
En est√© inciso, al traer los datos, aprovech√© para dejar las columnas que me interesaban para el siguiente. 

En lo que respecta al armado del data set, podr√≠a haber trabajado con ...
  - informaci√≥n de cada producto del cat√°logo en s√≠ (por ejemplo ratings, como us√© los de vendedores)
  - las coordenadas de las ciudades, para agregar features de longitud/latitud, y poder representar mejor la ubicaci√≥n geogr√°fica (Ej: https://api.mercadolibre.com//classified_locations/cities/TUxBQkZMTzg5MjFa)

Adem√°s, podr√≠a haber aplicado alg√∫n m√©todo de [selecci√≥n de features](https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/). Lo que hice fue dejar aquellas que me parec√≠a ser√≠an de utilidad, considerando que siempre le dedico m√°s tiempo a adquirir conocimiento del dominio y ver los datos. Por ejemplo, podr√≠a haber realizado una matriz de correlaci√≥n con las features num√©ricas, para ver si alguna estaba fuertemente correlacionada con el target.


## üìå Modelo

### Data set

Cuando hice la selecci√≥n de features en el inciso anterior, dej√© algunas relacionadas a *tags*, y la que tiene el listado con todas las categor√≠as desde la root hasta la final. Despu√©s, en el notebook donde separo en train y test, las quit√©.

En un principio las hab√≠a seleccionado ya que quer√≠a probar codificar sus valores con embeddings. 

Para el caso de los tags, primero tendr√≠a que ver si hay muchos tags diferentes y distintas combinaciones como para que amerite a usar este enfoque. 

Para lo que es el camino de todas las categor√≠as, me parec√≠a m√°s intuitivo: me interesaba poder usar como feature no solo la categoria root, sino todas; para √©sto, no resulta viable el uso de dummies: hay demasiadas categor√≠as, y adem√°s, me gustaba el hecho de modelar la relaci√≥n de similitud/diferencia entre ellas. La idea era entrenar un embedding (puntualmente usando la librer√≠a gensim, y el modelo [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)) para que represente en el espacio de vectores una a una las categor√≠as, teniendo en cuenta su contexto, y luego hacer un promedio para tener la representaci√≥n final de todo ese camino. Me parec√≠a que pod√≠a sumar a la hora de elegir features que caractericen a cada publicaci√≥n.

Cuando vi el atributo 't√≠tulo' tambi√©n se me vino a la mente el uso de embeddings, pero en este caso hab√≠a una etapa muy fuerte de limpieza (considerando faltas de ortografia, abreviaturas, uso de tildes, etc).

Si llegaba a conseguir estas representaciones con embeddings, lo que faltaba era agregar una a una las dimensiones de los vectores como nuevas featrues para el clasificador, considerando cada dimension modela cierto aspecto.


### Training

A la hora de elegir un m√©todo de optimizaci√≥n para la selecci√≥n de hyper par√°metros, opt√© por *Grid Search*, pero tambi√©n podr√≠a haber utilizado *Random Search* y *Bayes*.

Adem√°s, podr√≠a haber agregado m√°s configuraciones en el listado a probar.

Us√© *SMOTE* para oversampling, y tambi√©n podr√≠a haber probado *NearMiss* para hacer un undersampling de la clase mayoritaria.

La funci√≥n de error a optimizar que eleg√≠ es **f1_macro**, ya que resume el *precision* y *recall* de cada clase y al ser ['macro' sirve para problemas de desbalanceo](https://stackoverflow.com/a/53197497/9932220).


### Evaluaci√≥n

Nota: m√°s all√° de que el notebook de evaluaci√≥n es totalmente configurable a la hora de elegir el data set a usar, lo repliqu√© para que queden por un lado los resultados con el data set de test, y por el otro los de train.

Lo que esperaba ver, aparte del resultado del accuracy (no suele ser tan alto en este tipo de problemas, con tantas clases), es que al visualizar la matriz de confusi√≥n, "se marque la diagonal", y para cada clase, se vaya viendo un "degrad√©" hacia los costados; es decir, que el modelo le acierte a la gran mayoria de casos, y que si se equivoca, que sea mayoritariamente con clases cercanas. Para ilustrar un caso extremo, no estar√≠a bueno que se confunda la clase m√°s alta con la m√°s baja por ejemplo, ya que no deber√≠an parecerse.

Viendo los resulados para ambos data sets, queda en evidencia que tanto para el *XGBoost* como el *MLP Classifier*, estamos ante la presencia de *overfitting*. Los modelos no generalizan lo visto en los datos de entrenamiento, y ante datos nuevos, responden mal. Para solucionar esto, incorporar√≠a regularizaci√≥n. Esto implicar√≠a agregar nuevos valores a probar para ciertos hyperpar√°metros [(1)](https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_alpha.html) [(2)](https://medium.com/data-design/xgboost-hi-im-gamma-what-can-i-do-for-you-and-the-tuning-of-regularization-a42ea17e6ab6).

Para el caso de *Logistic Regression*, nos damos cuenta ni siquiera logr√≥ modelar la data de entrenamiento. Estamos ante un caso de *underfitting*.

Por √∫ltimo, para lo que es la *interpretabilidad* de los modelos, la librer√≠a [SHAP](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html) resulta interesante a la hora de entender como funciona el modelo en general (features importance), y para un caso puntual.